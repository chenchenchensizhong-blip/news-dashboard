import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from openai import OpenAI
import os

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="å…¨ç½‘çƒ­ç‚¹ç›‘æ§ V3.6 (Llama 3.3)", page_icon="ğŸš€", layout="wide")

st.markdown("""
    <style>
    .block-container {padding-top: 1rem; padding-bottom: 2rem;}
    .ai-report {
        background-color: #f0f2f6; 
        padding: 20px; 
        border-radius: 10px; 
        border-left: 5px solid #00b894;
        font-family: 'Microsoft YaHei', sans-serif;
    }
    div[data-testid="stVerticalBlock"] > div {gap: 0.5rem;}
    p {margin-bottom: 0.2rem;}
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸš€ å…¨ç½‘çƒ­ç‚¹ç›‘æ§ä¸­å¿ƒ (V3.6 æœ€æ–°æ¨¡å‹ç‰ˆ)")
st.caption("å·²å‡çº§è‡³ Llama 3.3 70B | æé€Ÿå“åº” | å…¼å®¹ DeepSeek/Kimi")

# --- 0. æ§åˆ¶å° & è®¾ç½® ---
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶å°")
    if st.button("ğŸ”„ åˆ·æ–°å…¨ç½‘æ•°æ®", use_container_width=True, type="primary"):
        st.cache_data.clear()
        st.rerun()
    
    st.markdown("---")
    st.header("ğŸ¤– AI é…ç½® (Groq)")
    
    # é»˜è®¤åœ°å€
    api_base = st.text_input("API Base URL", value="https://api.groq.com/openai/v1")
    
    api_key = st.text_input("API Key", type="password", help="åœ¨æ­¤å¡«å…¥ Groq çš„ gsk_... Key")
    
    # === å…³é”®ä¿®æ”¹ï¼šæ›´æ–°ä¸º Llama 3.3 æœ€æ–°æ¨¡å‹ ===
    # æ—§çš„ llama3-70b-8192 å·²ä¸‹æ¶
    # æ–°çš„æ¨èæ¨¡å‹æ˜¯: llama-3.3-70b-versatile
    model_name = st.text_input("æ¨¡å‹åç§°", value="llama-3.3-70b-versatile")
    
    st.markdown("---")
    st.header("ğŸŒ ç½‘ç»œè®¾ç½®")
    proxy_port = st.text_input("æœ¬åœ°ä»£ç†ç«¯å£ (VPN)", value="7897")
    
    if proxy_port:
        proxy_url = f"http://127.0.0.1:{proxy_port}"
        PROXIES = {"http": proxy_url, "https": proxy_url}
        st.success(f"çˆ¬è™«ä»£ç†: {proxy_port}")
        
        # å¼ºåˆ¶æ³¨å…¥ AI ä»£ç†
        os.environ["HTTP_PROXY"] = proxy_url
        os.environ["HTTPS_PROXY"] = proxy_url
        st.success(f"AI ä»£ç†: {proxy_port} (ç¯å¢ƒæ³¨å…¥)")
    else:
        PROXIES = None
        os.environ.pop("HTTP_PROXY", None)
        os.environ.pop("HTTPS_PROXY", None)
        st.warning("æ— ä»£ç†")

def get_html(url, use_proxy=False):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
    }
    try:
        p = PROXIES if use_proxy else None
        response = requests.get(url, headers=headers, proxies=p, timeout=10)
        response.encoding = 'utf-8'
        return response.text if response.status_code == 200 else None
    except: return None

# --- 1. çˆ¬è™«æ¨¡å— ---

@st.cache_data(ttl=3600)
def scrape_baidu():
    url = "https://top.baidu.com/board?tab=realtime"
    html = get_html(url)
    if not html: return pd.DataFrame()
    soup = BeautifulSoup(html, 'lxml')
    data = []
    items = soup.find_all('div', class_='category-wrap_iQLoo')
    for idx, item in enumerate(items[:10]):
        try:
            title = item.find('div', class_='c-single-text-ellipsis').text.strip()
            link = item.find('a')['href']
            heat = item.find('div', class_='hot-index_1Bl1a').text.strip()
            data.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": link, "çƒ­åº¦": heat})
        except: continue
    return pd.DataFrame(data)

@st.cache_data(ttl=3600)
def scrape_weibo():
    api_url = "https://weibo.com/ajax/side/hotSearch"
    try:
        headers = {"User-Agent": "Mozilla/5.0", "Referer": "https://weibo.com/"}
        resp = requests.get(api_url, headers=headers, timeout=5)
        data = resp.json()
        realtime_list = data['data']['realtime']
        result = []
        for idx, item in enumerate(realtime_list[:10]):
            if 'rank' not in item and 'is_ad' in item: continue
            title = item['word']
            link = f"https://s.weibo.com/weibo?q={title}"
            heat = item.get('num', 'ç½®é¡¶')
            tag = item.get('label_name', '')
            desc = f"ã€{tag}ã€‘" if tag else ""
            result.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": link, "çƒ­åº¦": str(heat), "ç®€ä»‹": desc})
        return pd.DataFrame(result)
    except: return pd.DataFrame()

@st.cache_data(ttl=3600)
def scrape_bilibili():
    api_url = "https://api.bilibili.com/x/web-interface/ranking/v2?rid=0&type=all"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Referer": "https://www.bilibili.com/v/popular/rank/all"
    }
    try:
        resp = requests.get(api_url, headers=headers, timeout=5)
        json_data = resp.json()
        video_list = json_data['data']['list']
        data = []
        for idx, video in enumerate(video_list[:10]):
            title = video['title']
            author = video['owner']['name']
            play = video['stat']['view']
            play_str = f"{play/10000:.1f}ä¸‡" if play > 10000 else str(play)
            link = video['short_link_v2']
            data.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": link, "UPä¸»": author, "æ’­æ”¾": play_str})
        return pd.DataFrame(data)
    except: return pd.DataFrame()

@st.cache_data(ttl=3600)
def scrape_overseas(platform):
    url = "https://kworb.net/youtube/trending_overall.html" if platform == "youtube" else "https://getdaytrends.com/"
    html = get_html(url, use_proxy=True)
    if not html: return pd.DataFrame()
    soup = BeautifulSoup(html, 'lxml')
    data = []
    
    if platform == "youtube":
        try:
            rows = soup.find('tbody').find_all('tr')
            for idx, row in enumerate(rows[:10]):
                link_tag = row.find('a')
                if link_tag:
                    title = link_tag.text.strip()
                    raw_href = link_tag['href']
                    link = f"https://www.youtube.com/watch?v={raw_href.split('video/')[1].replace('.html','')}" if "video/" in raw_href else "https://www.youtube.com"+raw_href
                    data.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": link})
        except: pass
    elif platform == "x":
        try:
            rows = soup.select('table.table tbody tr')
            for idx, row in enumerate(rows[:10]):
                link_tag = row.find('a')
                if link_tag:
                    title = link_tag.text.strip()
                    link = "https://twitter.com/search?q=" + title.replace("#", "%23")
                    heat = row.find('small').text.strip() if row.find('small') else ""
                    data.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": link, "çƒ­åº¦": heat})
        except: pass
    return pd.DataFrame(data)

# --- 2. AI åˆ†ææ¨¡å— ---

def generate_ai_report(dfs_dict, api_key, api_base, model_name):
    if not api_key:
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥ API Key å¼€å¯ AI åˆ†æ")
        return

    prompt_text = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å…¨ç½‘èˆ†æƒ…åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯å½“å‰å„å¤§å¹³å°çš„çƒ­æœå‰10åæ•°æ®ï¼š\n\n"
    for platform, df in dfs_dict.items():
        if not df.empty:
            titles = df['æ ‡é¢˜'].tolist()
            prompt_text += f"ã€{platform}ã€‘ï¼š{', '.join(titles)}\n"
    
    prompt_text += """
    \nè¯·æ ¹æ®ä»¥ä¸Šæ•°æ®ï¼Œç”¨ä¸­æ–‡ç”Ÿæˆä¸€ä»½ç®€æŠ¥ï¼ˆMarkdownæ ¼å¼ï¼‰ï¼š
    1. **å…¨ç½‘æ ¸å¿ƒç„¦ç‚¹**ï¼šç”¨ä¸€å¥è¯æ€»ç»“å½“å‰ä¸è®ºå›½å†…è¿˜æ˜¯å›½å¤–ï¼Œå¤§å®¶æœ€å…³æ³¨çš„ä¸€ä»¶äº‹ã€‚
    2. **æƒ…ç»ªæ™´é›¨è¡¨**ï¼šå½“å‰ç½‘æ°‘æ•´ä½“æƒ…ç»ªæ˜¯ç„¦è™‘ã€å¨±ä¹ã€æ„¤æ€’è¿˜æ˜¯å¹³é™ï¼Ÿ
    3. **å·®å¼‚åŒ–æ´å¯Ÿ**ï¼šå›½å†…å¹³å°ä¸æµ·å¤–å¹³å°å…³æ³¨ç‚¹çš„æœ€å¤§åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
    4. **çˆ†æ¬¾é¢„æµ‹**ï¼šé¢„æµ‹å“ªä¸€ä¸ªè¯é¢˜æœ€æœ‰å¯èƒ½åœ¨æ¥ä¸‹æ¥å‡ å°æ—¶å†…æŒç»­å‘é…µï¼Ÿ
    """

    try:
        client = OpenAI(api_key=api_key, base_url=api_base)
        
        with st.spinner(f"ğŸš€ æ­£åœ¨å‘¼å« {model_name} è¿›è¡Œå…‰é€Ÿåˆ†æ..."):
            completion = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„æ•°æ®åˆ†æåŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt_text}
                ],
                temperature=0.7,
            )
            ai_content = completion.choices[0].message.content
            
            st.markdown('<div class="ai-report">', unsafe_allow_html=True)
            st.markdown("### ğŸš€ AI å…¨ç½‘èˆ†æƒ…æ·±åº¦ç®€æŠ¥")
            st.markdown(ai_content)
            st.markdown('</div>', unsafe_allow_html=True)
            
    except Exception as e:
        st.error(f"âŒ AI åˆ†æå¤±è´¥: {e}")
        st.warning("æç¤ºï¼šå¦‚æœé‡åˆ° 'model decommissioned' é”™è¯¯ï¼Œè¯·åœ¨ä¾§è¾¹æ æ‰‹åŠ¨å°†æ¨¡å‹åç§°æ”¹ä¸º 'llama-3.3-70b-versatile'")

# --- 3. UI æ¸²æŸ“ ---

def render_column(title, emoji, df):
    with st.container():
        st.markdown(f"### {emoji} {title}")
        st.markdown("---")
        if df.empty:
            st.warning("æš‚æ— æ•°æ®")
        else:
            for _, row in df.iterrows():
                st.markdown(f"**{row['æ’å']}. [{row['æ ‡é¢˜']}]({row['é“¾æ¥']})**")
                meta = []
                if 'çƒ­åº¦' in row and row['çƒ­åº¦']: meta.append(f"ğŸ”¥ {row['çƒ­åº¦']}")
                if 'UPä¸»' in row: meta.append(f"ğŸ‘¤ {row['UPä¸»']}")
                if 'ç®€ä»‹' in row and row['ç®€ä»‹']: meta.append(f"{row['ç®€ä»‹']}")
                st.caption(" Â· ".join(meta))
                st.markdown("---")

# --- ä¸»ç¨‹åº ---
df_baidu = scrape_baidu()
df_weibo = scrape_weibo()
df_bili = scrape_bilibili()
df_yt = scrape_overseas("youtube") if PROXIES else pd.DataFrame()
df_x = scrape_overseas("x") if PROXIES else pd.DataFrame()

c1, c2, c3 = st.columns(3)
with c1: render_column("ç™¾åº¦çƒ­æœ", "ğŸ‡¨ğŸ‡³", df_baidu)
with c2: render_column("å¾®åšçƒ­æœ", "ğŸ‡¨ğŸ‡³", df_weibo)
with c3: render_column("Bç«™çƒ­é—¨", "ğŸ“º", df_bili)

st.markdown("<br>", unsafe_allow_html=True)

c4, c5, c6 = st.columns(3)
with c4:
    if PROXIES: render_column("YouTube", "ğŸŸ¥", df_yt)
    else: st.error("éœ€ä»£ç†")
with c5:
    if PROXIES: render_column("Twitter (X)", "âœ–ï¸", df_x)
    else: st.error("éœ€ä»£ç†")
with c6:
    all_data = {"ç™¾åº¦": df_baidu, "å¾®åš": df_weibo, "Bç«™": df_bili, "YouTube": df_yt, "Twitter": df_x}
    # ä»ä¾§è¾¹æ è·å–é…ç½®ï¼Œå¹¶ç›´æ¥è°ƒç”¨
    generate_ai_report(all_data, api_key, api_base, model_name)