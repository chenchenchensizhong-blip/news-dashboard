import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
# [ä¿®æ”¹ 1] å¼•å…¥æ™ºè°±AIå®˜æ–¹SDK
from zhipuai import ZhipuAI 
import os
import random
import datetime
import json
import time

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="å…¨ç½‘çƒ­ç‚¹ V4.4 (æ™ºè°±ç‰ˆ)", page_icon="ğŸ”¥", layout="wide")

st.markdown("""
    <style>
    .block-container {padding-top: 1rem; padding-bottom: 2rem;}
    /* AI æŠ¥å‘Šæ ·å¼ä¼˜åŒ–ï¼Œé€‚åº”ç«–å‘æ’ç‰ˆ */
    .ai-report {
        background-color: #f0f2f6; 
        padding: 15px; 
        border-radius: 10px; 
        border-top: 5px solid #3498db; /* æ™ºè°±è“ */
        font-family: 'Microsoft YaHei', sans-serif;
        font-size: 14px;
    }
    div[data-testid="stVerticalBlock"] > div {gap: 0.5rem;}
    .demo-tag {
        font-size: 12px; color: #ff9f43; background: #fff3cd; 
        padding: 2px 6px; border-radius: 4px; margin-left: 5px;
    }
    a {text-decoration: none;}
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸ”¥ å…¨ç½‘çƒ­ç‚¹ç›‘æ§ (V4.4 æ™ºè°±é€‚é…ç‰ˆ)")
st.caption("8å¤§æ¨¡å—èšåˆ | 4x2 é»„é‡‘ç½‘æ ¼ | æ™ºè°± GLM å®æ—¶æ´å¯Ÿ")

# --- 0. æ§åˆ¶å° & è®¾ç½® ---
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶å°")
    if st.button("ğŸ”„ åˆ·æ–°å…¨ç½‘æ•°æ®", use_container_width=True, type="primary"):
        st.cache_data.clear()
        st.rerun()
    
    st.markdown("---")
    st.header("ğŸ¤– æ™ºè°± AI é…ç½®")
    # [ä¿®æ”¹ 2] ç§»é™¤Base URLï¼Œä»…ä¿ç•™API Keyå’Œæ¨¡å‹åç§°
    api_key = st.text_input("æ™ºè°± API Key", type="password", help="è¯·å‰å¾€ bigmodel.cn è·å–")
    model_name = st.text_input("æ¨¡å‹åç§°", value="glm-4-flash", help="æ¨è glm-4-flash (å¿«) æˆ– glm-4")
    
    st.markdown("---")
    st.header("ğŸŒ ç½‘ç»œè®¾ç½®")
    is_cloud_mode = st.checkbox("æˆ‘æ˜¯äº‘ç«¯éƒ¨ç½² (Cloud Mode)", value=True)
    proxy_port = st.text_input("æœ¬åœ°ä»£ç†ç«¯å£", value="")
    
    PROXIES = None
    if proxy_port:
        proxy_url = f"http://127.0.0.1:{proxy_port}"
        PROXIES = {"http": proxy_url, "https": proxy_url}
        os.environ["HTTP_PROXY"] = proxy_url
        os.environ["HTTPS_PROXY"] = proxy_url
    else:
        os.environ.pop("HTTP_PROXY", None)
        os.environ.pop("HTTPS_PROXY", None)

# --- é€šç”¨å·¥å…· ---
def get_random_ua():
    return random.choice([
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    ])

def get_html(url, use_proxy=False, extra_headers=None):
    headers = {
        "User-Agent": get_random_ua(),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    }
    if extra_headers: headers.update(extra_headers)
    
    try:
        p = PROXIES if (use_proxy and not is_cloud_mode) else None
        response = requests.get(url, headers=headers, proxies=p, timeout=15)
        response.encoding = 'utf-8'
        return response.text if response.status_code == 200 else None
    except: return None

# --- 1. æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨ ---
def get_mock_data(platform_name):
    mock_db = {
        "ç™¾åº¦": ["ä¸­å›½ç©ºé—´ç«™", "GDPç›®æ ‡", "æ–‡æ—…æŠ¢äºº", "å›½äº§å¤§æ¨¡å‹", "äº”ä¸€è½¦ç¥¨"],
        "å¾®åš": ["å¾®åšåçˆ¬å‡çº§ä¸­", "å»ºè®®ç¨ååˆ·æ–°", "æ­£åœ¨å°è¯•ç ´è§£", "æ¼”ç¤ºæ•°æ®A", "æ¼”ç¤ºæ•°æ®B"],
        "Bç«™": ["ä½•åŒå­¦æ–°ä½œ", "ç½—ç¿”è¯´åˆ‘æ³•", "åŸç¥å‰ç»", "æ‹œå¹´çºª", "æ¼”ç¤ºæ•°æ®"],
        "æŠ–éŸ³": ["ç§‘ç›®ä¸‰", "çŒ«å’ªåç©ºç¿»", "ç‰¹ç§å…µæ—…æ¸¸", "å¬åŠæ”¹é€ "],
        "å°çº¢ä¹¦": ["å¹´åº¦æ€»ç»“", "æ˜¾çœ¼åŒ…ç©¿æ­", "CityWalk", "å‡è„‚é¤"],
        "YouTube": ["MrBeast", "GTA VI", "SpaceX"],
        "Twitter": ["#Bitcoin", "#AI", "Elon Musk"]
    }
    titles = mock_db.get(platform_name, ["çƒ­ç‚¹è¯é¢˜"])
    data = []
    for i in range(10):
        title = random.choice(titles) if i < len(titles) else f"{platform_name} çƒ­é—¨ {i+1}"
        data.append({
            "æ’å": i+1, "æ ‡é¢˜": title, "é“¾æ¥": "#", 
            "çƒ­åº¦": f"{random.randint(100,999)}w", 
            "ç®€ä»‹": "âš ï¸ æŠ“å–å¤±è´¥ (Mock)", "is_mock": True
        })
    return pd.DataFrame(data)

# --- 2. çˆ¬è™«æ¨¡å— ---

@st.cache_data(ttl=3600)
def scrape_baidu():
    html = get_html("https://top.baidu.com/board?tab=realtime")
    if not html: return get_mock_data("ç™¾åº¦")
    soup = BeautifulSoup(html, 'lxml')
    data = []
    items = soup.find_all('div', class_='category-wrap_iQLoo')
    for idx, item in enumerate(items[:10]):
        try:
            title = item.find('div', class_='c-single-text-ellipsis').text.strip()
            link = item.find('a')['href']
            heat = item.find('div', class_='hot-index_1Bl1a').text.strip()
            data.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": link, "çƒ­åº¦": heat, "is_mock": False})
        except: continue
    return pd.DataFrame(data) if data else get_mock_data("ç™¾åº¦")

@st.cache_data(ttl=3600)
def scrape_weibo():
    session = requests.Session()
    session.headers.update({"User-Agent": get_random_ua(), "Referer": "https://weibo.com/"})
    try:
        session.get("https://weibo.com/", timeout=5)
        resp = session.get("https://weibo.com/ajax/side/hotSearch", timeout=5)
        if resp.status_code == 200:
            data = resp.json()['data']['realtime']
            result = []
            for idx, item in enumerate(data[:10]):
                if 'rank' not in item and 'is_ad' in item: continue
                title = item['word']
                desc = f"ã€{item.get('label_name','')}ã€‘" if item.get('label_name') else ""
                result.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": f"https://s.weibo.com/weibo?q={title}", "çƒ­åº¦": str(item.get('num','')), "ç®€ä»‹": desc, "is_mock": False})
            if result: return pd.DataFrame(result)
    except: pass

    try:
        headers_mobile = {"User-Agent": "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Mobile Safari/537.36"}
        api_url = "https://m.weibo.cn/api/container/getIndex?containerid=106003type%3D25%26t%3D3%26disable_hot%3D1%26is_ext%3D1"
        resp = requests.get(api_url, headers=headers_mobile, timeout=5)
        if resp.status_code == 200:
            cards = resp.json()['data']['cards'][0]['card_group']
            result = []
            for idx, card in enumerate(cards[:10]):
                title = card['desc']
                result.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": card['scheme'], "çƒ­åº¦": str(card.get('desc_extr', '')), "ç®€ä»‹": "", "is_mock": False})
            if result: return pd.DataFrame(result)
    except: pass
    return get_mock_data("å¾®åš")

@st.cache_data(ttl=3600)
def scrape_bilibili():
    headers = {"User-Agent": get_random_ua(), "Referer": "https://www.bilibili.com/v/popular/rank/all", "Cookie": "b_nut=1712000000;"}
    try:
        resp = requests.get("https://api.bilibili.com/x/web-interface/ranking/v2?rid=0&type=all", headers=headers, timeout=5)
        if resp.status_code == 200:
            data = []
            for idx, v in enumerate(resp.json()['data']['list'][:10]):
                play = v['stat']['view']
                play_s = f"{play/10000:.1f}ä¸‡" if play > 10000 else str(play)
                data.append({"æ’å": idx+1, "æ ‡é¢˜": v['title'], "é“¾æ¥": v['short_link_v2'], "UPä¸»": v['owner']['name'], "æ’­æ”¾": play_s, "is_mock": False})
            return pd.DataFrame(data)
    except: pass
    return get_mock_data("Bç«™")

@st.cache_data(ttl=3600)
def scrape_douyin():
    try:
        url = "https://www.iesdouyin.com/web/api/v2/hotsearch/billboard/word/"
        resp = requests.get(url, headers={"User-Agent": get_random_ua()}, timeout=5)
        data = []
        for idx, item in enumerate(resp.json()['word_list'][:10]):
            title = item.get('word')
            heat = item.get('hot_value')
            data.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": f"https://www.douyin.com/search/{title}", "çƒ­åº¦": f"{heat/10000:.1f}w", "is_mock": False})
        return pd.DataFrame(data)
    except: return get_mock_data("æŠ–éŸ³")

@st.cache_data(ttl=3600)
def scrape_xhs():
    return get_mock_data("å°çº¢ä¹¦")

@st.cache_data(ttl=3600)
def scrape_overseas(platform):
    url = "https://kworb.net/youtube/trending_overall.html" if platform == "youtube" else "https://getdaytrends.com/"
    html = get_html(url, use_proxy=True)
    if not html: return get_mock_data("YouTube" if platform=="youtube" else "Twitter")
    
    soup = BeautifulSoup(html, 'lxml')
    data = []
    try:
        if platform == "youtube":
            for idx, row in enumerate(soup.find('tbody').find_all('tr')[:10]):
                link_tag = row.find('a')
                href = link_tag['href']
                link = f"https://www.youtube.com/watch?v={href.split('video/')[1].replace('.html','')}" if "video/" in href else href
                data.append({"æ’å": idx+1, "æ ‡é¢˜": link_tag.text.strip(), "é“¾æ¥": link, "is_mock": False})
        elif platform == "x":
            rows = soup.select('table.table tbody tr')
            for idx, row in enumerate(rows[:10]):
                link_tag = row.find('a')
                if link_tag:
                    title = link_tag.text.strip()
                    link = "https://twitter.com/search?q=" + title.replace("#", "%23")
                    heat = row.find('small').text.strip() if row.find('small') else ""
                    data.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": link, "çƒ­åº¦": heat, "is_mock": False})
    except: pass
    return pd.DataFrame(data) if data else get_mock_data("YouTube" if platform=="youtube" else "Twitter")

# --- 4. AI åˆ†æ (é€‚é…æ™ºè°±ç‰ˆ) ---
def generate_ai_report(dfs_dict, api_key, model_name):
    # æ˜¾ç¤ºæ ‡é¢˜ï¼ˆä¸å…¶ä»–åˆ—å¯¹é½ï¼‰
    st.markdown("### ğŸ§  æ™ºè°± AI æ´å¯Ÿ")
    st.markdown("---")
    
    if not api_key:
        st.info("ğŸ‘ˆ è¯·é…ç½® æ™ºè°± API Key")
        return
    
    # æ„é€  Prompt
    prompt = "ä½ æ˜¯ä¸€ä½å…¨ç½‘èˆ†æƒ…ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯å„å¹³å°å®æ—¶çƒ­æœï¼š\n\n"
    has_data = False
    for plat, df in dfs_dict.items():
        if not df.empty:
            has_data = True
            titles = df['æ ‡é¢˜'].tolist()
            is_mock = df.iloc[0].get('is_mock', False)
            tag = "(æ¼”ç¤ºæ•°æ®)" if is_mock else ""
            prompt += f"ã€{plat}{tag}ã€‘ï¼š{', '.join(titles)}\n"
    
    if not has_data: return

    prompt += """
    \nè¯·ç”Ÿæˆä¸€ä»½ç®€ç»ƒçš„ã€èˆ†æƒ…ç®€æŠ¥ã€‘ï¼ˆMarkdownæ ¼å¼ï¼Œä¸è¦å¤ªé•¿ï¼‰ï¼š
    1. **ç„¦ç‚¹è¯é¢˜**ï¼šå…¨ç½‘éƒ½åœ¨çœ‹ä»€ä¹ˆï¼Ÿ
    2. **å¹³å°å·®å¼‚**ï¼šæŠ–éŸ³/å°çº¢ä¹¦ vs å¾®åš/Bç«™ vs æµ·å¤–ã€‚
    3. **è¶‹åŠ¿é¢„æµ‹**ï¼šä¸‹ä¸€ä¸ªçˆ†ç‚¹ã€‚
    """
    
    try:
        # [ä¿®æ”¹ 3] ä½¿ç”¨ ZhipuAI Client
        client = ZhipuAI(api_key=api_key)
        
        with st.spinner(f"ğŸš€ æ™ºè°± AI ({model_name}) åˆ†æä¸­..."):
            completion = client.chat.completions.create(
                model=model_name, 
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            st.markdown('<div class="ai-report">', unsafe_allow_html=True)
            st.markdown(completion.choices[0].message.content)
            st.markdown('</div>', unsafe_allow_html=True)
    except Exception as e: st.error(f"AI å¤±è´¥: {e}")

# --- 5. UI æ¸²æŸ“ ---
def render_col(title, emoji, df):
    with st.container():
        is_mock = df.iloc[0].get('is_mock', False) if not df.empty else False
        header = f"### {emoji} {title}"
        if is_mock: header += ' <span class="demo-tag">æ¼”ç¤º</span>'
        st.markdown(header, unsafe_allow_html=True)
        st.markdown("---")
        
        if df.empty: st.caption("æš‚æ— æ•°æ®")
        else:
            for _, row in df.iterrows():
                st.markdown(f"**{row['æ’å']}. [{row['æ ‡é¢˜']}]({row['é“¾æ¥']})**")
                meta = []
                if 'çƒ­åº¦' in row: meta.append(f"ğŸ”¥ {row['çƒ­åº¦']}")
                if 'UPä¸»' in row: meta.append(f"ğŸ‘¤ {row['UPä¸»']}")
                st.caption(" Â· ".join(meta))
                st.markdown("---")

# --- ä¸»ç¨‹åº ---
run_overseas = True if is_cloud_mode else (PROXIES is not None)

data_map = {
    "ç™¾åº¦": scrape_baidu(),
    "å¾®åš": scrape_weibo(),
    "Bç«™": scrape_bilibili(),
    "æŠ–éŸ³": scrape_douyin(),
    "å°çº¢ä¹¦": scrape_xhs(),
    "YouTube": scrape_overseas("youtube") if run_overseas else pd.DataFrame(),
    "Twitter": scrape_overseas("x") if run_overseas else pd.DataFrame()
}

# === å¸ƒå±€ï¼š4 + 4 å®Œç¾ç½‘æ ¼ ===

# ç¬¬ä¸€è¡Œï¼šå¾®åš | æŠ–éŸ³ | ç™¾åº¦ | Bç«™
c1, c2, c3, c4 = st.columns(4)
with c1: render_col("å¾®åš", "ğŸ‰", data_map["å¾®åš"])
with c2: render_col("æŠ–éŸ³", "ğŸµ", data_map["æŠ–éŸ³"])
with c3: render_col("ç™¾åº¦", "ğŸ‡¨ğŸ‡³", data_map["ç™¾åº¦"])
with c4: render_col("Bç«™", "ğŸ“º", data_map["Bç«™"])

st.markdown("<br>", unsafe_allow_html=True)

# ç¬¬äºŒè¡Œï¼šå°çº¢ä¹¦ | Twitter | YouTube | AIç®€æŠ¥
c5, c6, c7, c8 = st.columns(4)
with c5: render_col("å°çº¢ä¹¦", "ğŸ“•", data_map["å°çº¢ä¹¦"])

with c6:
    if run_overseas: render_col("Twitter", "âœ–ï¸", data_map["Twitter"])
    else: st.error("éœ€ä»£ç†")

with c7:
    if run_overseas: render_col("YouTube", "ğŸŸ¥", data_map["YouTube"])
    else: st.error("éœ€ä»£ç†")

with c8:
    # ç¬¬8åˆ—ä¸“é—¨æ”¾ AI æŠ¥å‘Š
    # [ä¿®æ”¹ 4] ç§»é™¤ api_base å‚æ•°
    generate_ai_report(data_map, api_key, model_name)