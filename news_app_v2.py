import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from openai import OpenAI
import os
import random
import datetime

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="å…¨ç½‘çƒ­ç‚¹ V3.8 (æ¼”ç¤ºç‰ˆ)", page_icon="ğŸ›¡ï¸", layout="wide")

st.markdown("""
    <style>
    .block-container {padding-top: 1rem; padding-bottom: 2rem;}
    .ai-report {
        background-color: #f0f2f6; 
        padding: 20px; 
        border-radius: 10px; 
        border-left: 5px solid #00b894;
        font-family: 'Microsoft YaHei', sans-serif;
    }
    div[data-testid="stVerticalBlock"] > div {gap: 0.5rem;}
    p {margin-bottom: 0.2rem;}
    .demo-tag {
        font-size: 12px; 
        color: #ff9f43; 
        background: #fff3cd; 
        padding: 2px 6px; 
        border-radius: 4px;
        margin-left: 5px;
    }
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸ›¡ï¸ å…¨ç½‘çƒ­ç‚¹ç›‘æ§ (V3.8 ä¼˜é›…é™çº§ç‰ˆ)")
st.caption("æ£€æµ‹åˆ°äº‘ç«¯é˜»æ–­æ—¶è‡ªåŠ¨åˆ‡æ¢è‡³æ¼”ç¤ºæ•°æ® | ä¿è¯ç•Œé¢å®Œæ•´æ€§")

# --- 0. æ§åˆ¶å° & è®¾ç½® ---
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶å°")
    if st.button("ğŸ”„ åˆ·æ–°å…¨ç½‘æ•°æ®", use_container_width=True, type="primary"):
        st.cache_data.clear()
        st.rerun()
    
    st.markdown("---")
    st.header("ğŸ¤– AI é…ç½®")
    api_base = st.text_input("API Base URL", value="https://api.groq.com/openai/v1")
    api_key = st.text_input("API Key", type="password")
    model_name = st.text_input("æ¨¡å‹åç§°", value="llama-3.3-70b-versatile")
    
    st.markdown("---")
    st.header("ğŸŒ ç½‘ç»œè®¾ç½®")
    is_cloud_mode = st.checkbox("æˆ‘æ˜¯äº‘ç«¯éƒ¨ç½² (Cloud Mode)", value=True)
    proxy_port = st.text_input("æœ¬åœ°ä»£ç†ç«¯å£ (ä»…æœ¬åœ°éœ€å¡«)", value="")
    
    PROXIES = None
    if proxy_port:
        proxy_url = f"http://127.0.0.1:{proxy_port}"
        PROXIES = {"http": proxy_url, "https": proxy_url}
        os.environ["HTTP_PROXY"] = proxy_url
        os.environ["HTTPS_PROXY"] = proxy_url
    else:
        os.environ.pop("HTTP_PROXY", None)
        os.environ.pop("HTTPS_PROXY", None)

def get_html(url, use_proxy=False):
    # å°½å¯èƒ½æ¨¡æ‹ŸçœŸå®ç”¨æˆ·
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Referer": "https://www.baidu.com/",
        "Cookie": "BIDUPSID=12345; PSTM=12345;" # å°è¯•å¡ä¸ªå‡ Cookie
    }
    
    try:
        p = PROXIES if (use_proxy and not is_cloud_mode) else None
        response = requests.get(url, headers=headers, proxies=p, timeout=10)
        response.encoding = 'utf-8'
        if response.status_code == 200:
            return response.text
        return None
    except:
        return None

# --- 1. æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨ (å…³é”®åŠŸèƒ½) ---
def get_mock_data(platform_name):
    """å½“çˆ¬è™«å¤±è´¥æ—¶ï¼Œç”Ÿæˆå¥½çœ‹çš„å‡æ•°æ®ï¼Œé˜²æ­¢å¼€å¤©çª—"""
    current_time = datetime.datetime.now().strftime("%H:%M")
    
    mock_titles = {
        "ç™¾åº¦": [
            f"ä¸­å›½ç©ºé—´ç«™ç¬¬å››æ‰¹èˆªå¤©å‘˜é€‰æ‹”å®Œæˆ {current_time}", "2024å¹´GDPå¢é•¿ç›®æ ‡å‘å¸ƒ", "å„åœ°æ–‡æ—…å¼€å¯'æŠ¢äºº'æ¨¡å¼",
            "å›½äº§å¤§æ¨¡å‹æŠ€æœ¯çªç ´", "æ–°èƒ½æºæ±½è½¦é”€é‡å†åˆ›æ–°é«˜", "äº”ä¸€å‡æœŸç«è½¦ç¥¨å¼€å”®", 
            "ç§‘å­¦å®¶å‘ç°æ–°ç³»å¤–è¡Œæ˜Ÿ", "æŸçŸ¥åæ­Œæ‰‹å·¡å›æ¼”å”±ä¼šå®˜å®£"
        ],
        "å¾®åš": [
            f"è¿™å°±æ˜¯ä¸­å›½å¼æµªæ¼« {current_time}", "å»ºè®®ä¸“å®¶ä¸è¦å»ºè®®", "è€ƒç ”æˆç»©", 
            "ç†ŠçŒ«èŠ±èŠ±", "æ˜¥å¤©çš„ç¬¬ä¸€æ¯å¥¶èŒ¶", "æ²¡æƒ³åˆ°ä½ æ˜¯è¿™æ ·çš„", 
            "å¯ä»¥ä¸ç»“å©šä½†ä¸èƒ½ä¸...", "è¿™æ³¼å¤©çš„å¯Œè´µè½®åˆ°æˆ‘äº†"
        ],
        "Bç«™": [
            "ã€ä½•åŒå­¦ã€‘æˆ‘åšäº†ä¸€ä¸ª...", "è€—æ—¶300å¤©ï¼Œè¿˜åŸ...", "ã€ç½—ç¿”ã€‘æ³•å¾‹...", 
            "è¿™æ˜¯æˆ‘ä¸èŠ±é’±èƒ½çœ‹çš„å—ï¼Ÿ", "åŸç¥ï¼šæ–°ç‰ˆæœ¬å‰ç»", "ã€å…¨ç¨‹é«˜èƒ½ã€‘...", 
            "2024æ‹œå¹´çºª", "å…³äºæˆ‘è½¬ç”Ÿå˜æˆ..."
        ]
    }
    
    titles = mock_titles.get(platform_name, ["æ¼”ç¤ºæ•°æ®æ ‡é¢˜1", "æ¼”ç¤ºæ•°æ®æ ‡é¢˜2"])
    data = []
    for i in range(8):
        title = random.choice(titles) if i < len(titles) else f"{platform_name}çƒ­ç‚¹è¯é¢˜ {i+1}"
        data.append({
            "æ’å": i+1,
            "æ ‡é¢˜": title,
            "é“¾æ¥": "#", # æ¼”ç¤ºé“¾æ¥
            "çƒ­åº¦": f"{random.randint(100, 999)}ä¸‡",
            "ç®€ä»‹": "âš ï¸ å› äº‘ç«¯IPé™åˆ¶ï¼Œå½“å‰æ˜¾ç¤ºä¸ºæ¼”ç¤ºæ•°æ® (Mock Data)",
            "is_mock": True # æ ‡è®°ä¸ºå‡æ•°æ®
        })
    return pd.DataFrame(data)

# --- 2. çˆ¬è™«æ¨¡å— (å¸¦é™çº§é€»è¾‘) ---

@st.cache_data(ttl=3600)
def scrape_baidu():
    url = "https://top.baidu.com/board?tab=realtime"
    html = get_html(url)
    if not html: return get_mock_data("ç™¾åº¦") # <--- å¤±è´¥åˆ™è¿”å›å‡æ•°æ®
    
    soup = BeautifulSoup(html, 'lxml')
    data = []
    items = soup.find_all('div', class_='category-wrap_iQLoo')
    for idx, item in enumerate(items[:10]):
        try:
            title = item.find('div', class_='c-single-text-ellipsis').text.strip()
            link = item.find('a')['href']
            heat = item.find('div', class_='hot-index_1Bl1a').text.strip()
            data.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": link, "çƒ­åº¦": heat, "is_mock": False})
        except: continue
        
    if not data: return get_mock_data("ç™¾åº¦")
    return pd.DataFrame(data)

@st.cache_data(ttl=3600)
def scrape_weibo():
    api_url = "https://weibo.com/ajax/side/hotSearch"
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
            "Referer": "https://weibo.com/",
            "Cookie": "SUB=_2A25;" # æç®€ Cookie
        }
        resp = requests.get(api_url, headers=headers, timeout=5)
        if resp.status_code != 200: return get_mock_data("å¾®åš")
        
        data = resp.json()
        realtime_list = data['data']['realtime']
        result = []
        for idx, item in enumerate(realtime_list[:10]):
            if 'rank' not in item and 'is_ad' in item: continue
            title = item['word']
            link = f"https://s.weibo.com/weibo?q={title}"
            heat = item.get('num', 'ç½®é¡¶')
            tag = item.get('label_name', '')
            desc = f"ã€{tag}ã€‘" if tag else ""
            result.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": link, "çƒ­åº¦": str(heat), "ç®€ä»‹": desc, "is_mock": False})
        return pd.DataFrame(result)
    except: return get_mock_data("å¾®åš")

@st.cache_data(ttl=3600)
def scrape_bilibili():
    # Bç«™ Web æ¥å£
    api_url = "https://api.bilibili.com/x/web-interface/ranking/v2?rid=0&type=all"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
        "Referer": "https://www.bilibili.com/"
    }
    try:
        resp = requests.get(api_url, headers=headers, timeout=5)
        if resp.status_code != 200: return get_mock_data("Bç«™")
        json_data = resp.json()
        video_list = json_data['data']['list']
        data = []
        for idx, video in enumerate(video_list[:10]):
            title = video['title']
            author = video['owner']['name']
            play = video['stat']['view']
            play_str = f"{play/10000:.1f}ä¸‡" if play > 10000 else str(play)
            link = video['short_link_v2']
            data.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": link, "UPä¸»": author, "æ’­æ”¾": play_str, "is_mock": False})
        return pd.DataFrame(data)
    except: return get_mock_data("Bç«™")

@st.cache_data(ttl=3600)
def scrape_overseas(platform):
    # æµ·å¤–å¹³å°ä»£ç ä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒä»¬åœ¨äº‘ç«¯æ˜¯é€šçš„
    url = "https://kworb.net/youtube/trending_overall.html" if platform == "youtube" else "https://getdaytrends.com/"
    html = get_html(url, use_proxy=True)
    if not html: return pd.DataFrame() # æµ·å¤–å¤±è´¥æš‚æ—¶ä¸ Mockï¼Œå› ä¸ºé€šå¸¸èƒ½é€š
    soup = BeautifulSoup(html, 'lxml')
    data = []
    
    if platform == "youtube":
        try:
            rows = soup.find('tbody').find_all('tr')
            for idx, row in enumerate(rows[:10]):
                link_tag = row.find('a')
                if link_tag:
                    title = link_tag.text.strip()
                    raw_href = link_tag['href']
                    link = f"https://www.youtube.com/watch?v={raw_href.split('video/')[1].replace('.html','')}" if "video/" in raw_href else "https://www.youtube.com"+raw_href
                    data.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": link, "is_mock": False})
        except: pass
    elif platform == "x":
        try:
            rows = soup.select('table.table tbody tr')
            for idx, row in enumerate(rows[:10]):
                link_tag = row.find('a')
                if link_tag:
                    title = link_tag.text.strip()
                    link = "https://twitter.com/search?q=" + title.replace("#", "%23")
                    heat = row.find('small').text.strip() if row.find('small') else ""
                    data.append({"æ’å": idx+1, "æ ‡é¢˜": title, "é“¾æ¥": link, "çƒ­åº¦": heat, "is_mock": False})
        except: pass
    return pd.DataFrame(data)

# --- 3. AI åˆ†ææ¨¡å— ---

def generate_ai_report(dfs_dict, api_key, api_base, model_name):
    if not api_key:
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾“å…¥ API Key å¼€å¯ AI åˆ†æ")
        return

    prompt_text = "ä½ æ˜¯ä¸€ä½èˆ†æƒ…åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯æ•°æ®ï¼ˆéƒ¨åˆ†å¯èƒ½ä¸ºæ¼”ç¤ºæ•°æ®ï¼Œè¯·æ­£å¸¸åˆ†æï¼‰ï¼š\n\n"
    has_data = False
    for platform, df in dfs_dict.items():
        if not df.empty:
            has_data = True
            titles = df['æ ‡é¢˜'].tolist()
            # å¦‚æœæ˜¯å‡æ•°æ®ï¼Œç¨å¾®æç¤ºä¸€ä¸‹ AIï¼Œä½†è®©å®ƒç»§ç»­åˆ†æ
            is_mock = df.iloc[0].get('is_mock', False)
            note = "(æ¼”ç¤ºæ•°æ®)" if is_mock else ""
            prompt_text += f"ã€{platform}{note}ã€‘ï¼š{', '.join(titles)}\n"
    
    if not has_data: return

    prompt_text += """
    \nè¯·ç”Ÿæˆç®€æŠ¥ï¼ˆMarkdownï¼‰ï¼š
    1. **æ ¸å¿ƒç„¦ç‚¹**ï¼šæ€»ç»“å…³æ³¨ç‚¹ã€‚
    2. **æƒ…ç»ªæ™´é›¨è¡¨**ï¼šåˆ†ææƒ…ç»ªã€‚
    3. **çˆ†æ¬¾é¢„æµ‹**ï¼šé¢„æµ‹å‘é…µè¯é¢˜ã€‚
    """

    try:
        client = OpenAI(api_key=api_key, base_url=api_base)
        with st.spinner(f"ğŸš€ æ­£åœ¨å‘¼å« {model_name} åˆ†æ..."):
            completion = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt_text}],
                temperature=0.7,
            )
            st.markdown('<div class="ai-report">', unsafe_allow_html=True)
            st.markdown("### ğŸš€ AI èˆ†æƒ…ç®€æŠ¥")
            st.markdown(completion.choices[0].message.content)
            st.markdown('</div>', unsafe_allow_html=True)
    except Exception as e:
        st.error(f"AI åˆ†æå¤±è´¥: {e}")

# --- 4. UI æ¸²æŸ“ ---

def render_column(title, emoji, df):
    with st.container():
        # æ£€æŸ¥æ˜¯å¦æ˜¯ Mock æ•°æ®
        is_mock = False
        if not df.empty and 'is_mock' in df.columns:
            is_mock = df.iloc[0]['is_mock']
            
        header_html = f"### {emoji} {title}"
        if is_mock:
            header_html += ' <span class="demo-tag">æ¼”ç¤ºæ•°æ®</span>'
            
        st.markdown(header_html, unsafe_allow_html=True)
        st.markdown("---")
        
        if df.empty:
            st.caption("âš ï¸ æš‚æ— æ•°æ®")
        else:
            for _, row in df.iterrows():
                st.markdown(f"**{row['æ’å']}. [{row['æ ‡é¢˜']}]({row['é“¾æ¥']})**")
                
                meta = []
                if 'çƒ­åº¦' in row and row['çƒ­åº¦']: meta.append(f"ğŸ”¥ {row['çƒ­åº¦']}")
                if 'UPä¸»' in row: meta.append(f"ğŸ‘¤ {row['UPä¸»']}")
                if 'ç®€ä»‹' in row and row['ç®€ä»‹']: meta.append(f"{row['ç®€ä»‹']}")
                
                # å¦‚æœæ˜¯ Mock æ•°æ®ï¼Œè¯´æ˜åŸå› 
                if is_mock and row['æ’å'] == 1:
                    st.caption("âš ï¸ äº‘ç«¯IPè¢«æ‹¦æˆªï¼Œå·²è‡ªåŠ¨åˆ‡æ¢è‡³æ¼”ç¤ºæ•°æ®ä»¥ä¿æŒç•Œé¢å®Œæ•´ã€‚")
                else:
                    st.caption(" Â· ".join(meta))
                st.markdown("---")

# --- ä¸»ç¨‹åº ---
run_overseas = True if is_cloud_mode else (PROXIES is not None)

df_baidu = scrape_baidu()
df_weibo = scrape_weibo()
df_bili = scrape_bilibili()
df_yt = scrape_overseas("youtube") if run_overseas else pd.DataFrame()
df_x = scrape_overseas("x") if run_overseas else pd.DataFrame()

c1, c2, c3 = st.columns(3)
with c1: render_column("ç™¾åº¦çƒ­æœ", "ğŸ‡¨ğŸ‡³", df_baidu)
with c2: render_column("å¾®åšçƒ­æœ", "ğŸ‡¨ğŸ‡³", df_weibo)
with c3: render_column("Bç«™çƒ­é—¨", "ğŸ“º", df_bili)

st.markdown("<br>", unsafe_allow_html=True)

c4, c5, c6 = st.columns(3)
with c4:
    if run_overseas: render_column("YouTube", "ğŸŸ¥", df_yt)
    else: st.error("æœ¬åœ°éœ€é…ç½®ä»£ç†")
with c5:
    if run_overseas: render_column("Twitter (X)", "âœ–ï¸", df_x)
    else: st.error("æœ¬åœ°éœ€é…ç½®ä»£ç†")
with c6:
    all_data = {"ç™¾åº¦": df_baidu, "å¾®åš": df_weibo, "Bç«™": df_bili, "YouTube": df_yt, "Twitter": df_x}
    generate_ai_report(all_data, api_key, api_base, model_name)